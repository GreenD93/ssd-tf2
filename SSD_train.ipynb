{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from ssd.ssd_vgg16 import create_ssd_vgg16\n",
    "from ssd.ssd_mobilenetv1 import create_ssd_mobilenetv1\n",
    "from ssd.ssd_mobilenetv1_lite import create_ssd_mobilenetv1_lite\n",
    "from ssd.ssd_mobilenetv2_lite import create_ssd_mobilenetv2_lite\n",
    "from ssd.pre_ssd_mobilenetv1_lite import create_pre_ssd_mobilenetv1_lite\n",
    "from ssd.pre_ssd_mobilenetv2_lite import create_pre_ssd_mobilenetv2_lite\n",
    "from ssd.ssd import init_ssd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "from voc_data import create_batch_generator\n",
    "from anchor import generate_default_boxes\n",
    "from losses import create_losses\n",
    "\n",
    "from settings import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCH = 'pre_ssd300-mobilenetv2'\n",
    "CHECKPOINT_DIR = 'checkpoint/pre_mobilenetv2_lite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "default_boxes = generate_default_boxes(INFO[ARCH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator, val_generator, info = create_batch_generator(\n",
    "            DATA_DIR, DATA_YEAR, default_boxes,\n",
    "            SIZE, BATCH_SIZE, NUM_BATCHES,\n",
    "            mode='train', augmentation=['flip'])  # the patching algorithm is currently causing bottleneck sometimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd = create_pre_ssd_mobilenetv2_lite(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_type = 'new'\n",
    "net = init_ssd(ssd, pretrained_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = create_losses(NEG_RATIO, NUM_CLASSES)\n",
    "steps_per_epoch = info['length'] // BATCH_SIZE\n",
    "\n",
    "lr_fn = PiecewiseConstantDecay(\n",
    "        boundaries=[int(steps_per_epoch * NUM_EPOCHS * 2 / 3),\n",
    "                    int(steps_per_epoch * NUM_EPOCHS * 5 / 6)],\n",
    "        values=[INITIAL_LR, INITIAL_LR * 0.1, INITIAL_LR * 0.01])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "    learning_rate=INITIAL_LR,\n",
    "    momentum=MOMENTUM)\n",
    "\n",
    "train_log_dir = 'logs/train'\n",
    "val_log_dir = 'logs/val'\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(imgs, gt_confs, gt_locs, ssd, criterion, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        confs, locs = ssd(imgs)\n",
    "\n",
    "        conf_loss, loc_loss = criterion(\n",
    "            confs, locs, gt_confs, gt_locs)\n",
    "\n",
    "        loss = conf_loss + loc_loss\n",
    "        \n",
    "        #l2 regularization\n",
    "        l2_loss = [tf.nn.l2_loss(t) for t in ssd.trainable_variables]\n",
    "        l2_loss = WEIGHT_DECAY * tf.math.reduce_sum(l2_loss)\n",
    "        loss += l2_loss\n",
    "\n",
    "    gradients = tape.gradient(loss, ssd.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, ssd.trainable_variables))\n",
    "\n",
    "    return loss, conf_loss, loc_loss, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    avg_loss = 0.0\n",
    "    avg_conf_loss = 0.0\n",
    "    avg_loc_loss = 0.0\n",
    "    start = time.time()\n",
    "    \n",
    "    for i, (_, imgs, gt_confs, gt_locs) in enumerate(batch_generator):\n",
    "\n",
    "        loss, conf_loss, loc_loss, l2_loss = train_step(\n",
    "            imgs, gt_confs, gt_locs, ssd, criterion, optimizer)\n",
    "\n",
    "        avg_loss = (avg_loss * i + loss.numpy()) / (i + 1)\n",
    "        avg_conf_loss = (avg_conf_loss * i + conf_loss.numpy()) / (i + 1)\n",
    "        avg_loc_loss = (avg_loc_loss * i + loc_loss.numpy()) / (i + 1)\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print('Epoch: {} Batch {} Time: {:.2}s | Loss: {:.4f} Conf: {:.4f} Loc: {:.4f}'.format(\n",
    "                epoch + 1, i + 1, time.time() - start, avg_loss, avg_conf_loss, avg_loc_loss))\n",
    "\n",
    "    avg_val_loss = 0.0\n",
    "    avg_val_conf_loss = 0.0\n",
    "    avg_val_loc_loss = 0.0\n",
    "    \n",
    "    for i, (_, imgs, gt_confs, gt_locs) in enumerate(val_generator):\n",
    "        val_confs, val_locs = ssd(imgs)\n",
    "        val_conf_loss, val_loc_loss = criterion(\n",
    "            val_confs, val_locs, gt_confs, gt_locs)\n",
    "        \n",
    "        val_loss = val_conf_loss + val_loc_loss\n",
    "        avg_val_loss = (avg_val_loss * i + val_loss.numpy()) / (i + 1)\n",
    "        avg_val_conf_loss = (avg_val_conf_loss * i + val_conf_loss.numpy()) / (i + 1)\n",
    "        avg_val_loc_loss = (avg_val_loc_loss * i + val_loc_loss.numpy()) / (i + 1)\n",
    "    \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', avg_loss, step=epoch)\n",
    "        tf.summary.scalar('conf_loss', avg_conf_loss, step=epoch)\n",
    "        tf.summary.scalar('loc_loss', avg_loc_loss, step=epoch)\n",
    "\n",
    "    with val_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', avg_val_loss, step=epoch)\n",
    "        tf.summary.scalar('conf_loss', avg_val_conf_loss, step=epoch)\n",
    "        tf.summary.scalar('loc_loss', avg_val_loc_loss, step=epoch)\n",
    "\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        ssd.save_weights(\n",
    "            os.path.join(CHECKPOINT_DIR, 'ssd_epoch_{}.h5'.format(epoch + 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
